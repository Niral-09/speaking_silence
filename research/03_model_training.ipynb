{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/niralpatel/Desktop/Projects/speaking_silence'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    base_model_path: Path\n",
    "\n",
    "    training_data: Path\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    is_augmentation: bool\n",
    "    input_shape: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Speaking_Silence.constants import *\n",
    "from Speaking_Silence.utils.common import read_yaml, create_directories\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = \"artifacts/data_ingestion\"\n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            base_model_path=Path(prepare_base_model.base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            epochs=params.EPOCHS,\n",
    "            batch_size=params.BATCH_SIZE,\n",
    "            is_augmentation=params.AUGMENTATION,\n",
    "            input_shape=params.INPUT_SHAPE,\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            self.config.base_model_path\n",
    "        )\n",
    "\n",
    "    def train_valid_generator(self):\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.20\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.input_shape[:-1],\n",
    "            batch_size=self.config.batch_size,\n",
    "            interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs\n",
    "        )\n",
    "\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "        if self.config.is_augmentation:\n",
    "            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                rotation_range=40,\n",
    "                horizontal_flip=True,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                **datagenerator_kwargs\n",
    "            )\n",
    "        else:\n",
    "            train_datagenerator = valid_datagenerator\n",
    "\n",
    "        self.train_generator = train_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            subset=\"training\",\n",
    "            shuffle=True,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def train(self):\n",
    "        self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\n",
    "        self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size\n",
    "\n",
    "        self.model.fit(\n",
    "            self.train_generator,\n",
    "            epochs=self.config.epochs,\n",
    "            steps_per_epoch=self.steps_per_epoch,\n",
    "            validation_steps=self.validation_steps,\n",
    "            validation_data=self.valid_generator\n",
    "        )\n",
    "\n",
    "        self.save_model(self.config.trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = tf.keras.models.load_model(self.config.base_model_path)\n",
    "\n",
    "    def preprocess_video(self, video_path):\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        # Read frames from the video\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Preprocess frame (resize, normalize, etc.)\n",
    "            # Add any preprocessing steps needed here\n",
    "            frame = cv2.resize(frame, (224, 224))  # Example resizing to fit VGG16 input shape\n",
    "            frame = frame / 255.0  # Example normalization\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def train(self):\n",
    "        self.get_base_model()\n",
    "\n",
    "        # Iterate over video files in the training directory\n",
    "        for video_folder in os.listdir(self.config.training_data):\n",
    "            folder = f\"{self.config.training_data}/{video_folder}\"\n",
    "            for video_file in os.listdir(folder):\n",
    "                video_path = os.path.join(self.config.training_data, video_file)\n",
    "                frames = self.preprocess_video(video_path)\n",
    "\n",
    "            # Assuming 'frames' is a list of preprocessed frames from the video\n",
    "            # Perform any necessary additional preprocessing, such as padding frames or converting to a numpy array\n",
    "            # Then feed the frames into the model for training\n",
    "\n",
    "            # Example:\n",
    "            # frames = np.array(frames)\n",
    "            # self.model.fit(frames, ...)  # Example training step\n",
    "\n",
    "        # Save the trained model\n",
    "        self.model.save(self.config.trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-03-23 09:36:28: INFO: common]\u001b[0m yaml file: config/config.yaml loaded successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Speaking_Silence:yaml file: config/config.yaml loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-03-23 09:36:28: INFO: common]\u001b[0m yaml file: params.yaml loaded successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Speaking_Silence:yaml file: params.yaml loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-03-23 09:36:28: INFO: common]\u001b[0m created directory at: artifacts\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Speaking_Silence:created directory at: artifacts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-03-23 09:36:28: INFO: common]\u001b[0m created directory at: artifacts/training\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Speaking_Silence:created directory at: artifacts/training\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34746.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34744.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34741.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_67872.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34742.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34743.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34733.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34732.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34685.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34736.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_69395.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34737.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_66098.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_66099.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34734.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34739.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_34738.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_66097.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/man_68794.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26982.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26983.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_68350.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26981.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_65884.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26980.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26984.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26985.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26978.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26986.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_67750.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_68070.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_70016.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26974.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26975.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26977.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26976.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26972.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26973.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing_26971.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33277.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33274.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33271.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33270.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33266.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33273.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33267.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_70299.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33281.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_68093.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33280.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33269.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33282.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33283.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33268.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33278.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33286.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33279.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_69389.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like_33285.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08935.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_67465.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08936.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08937.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08950.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08944.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08945.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08951.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08947.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08952.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08946.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08942.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08955.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08948.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08949.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_69257.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08939.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_65294.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/can_08938.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55371.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55365.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55364.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55370.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55366.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55372.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55373.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55367.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55363.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55362.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_68624.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55361.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55375.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_68428.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55356.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_66575.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55369.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_68162.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/study_55368.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62152.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62168.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62169.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_66742.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_66743.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_68177.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62173.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62172.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62166.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_67036.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_68852.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_68918.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62158.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62170.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62164.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62165.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62171.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62159.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62175.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62160.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/walk_62163.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_65163.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_65162.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05641.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_65161.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05642.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_67400.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05644.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05637.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05636.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05634.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05635.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05631.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05630.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05632.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05633.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05629.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05628.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05638.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/bed_05639.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24940.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24954.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24955.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24941.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24969.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24943.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_67715.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24956.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24952.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24946.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24947.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24951.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24950.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24857.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_65824.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_68292.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_69345.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24961.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24960.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24948.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24962.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24973.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24970.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24971.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/go_24965.mp4\"\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'artifacts/data_ingestion/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     training \u001b[38;5;241m=\u001b[39m Training(config\u001b[38;5;241m=\u001b[39mtraining_config)\n\u001b[1;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_data):\n\u001b[1;32m     32\u001b[0m     folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     34\u001b[0m         video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_data, video_file)\n\u001b[1;32m     35\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_video(video_path)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'artifacts/data_ingestion/.DS_Store'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LRCN_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),\n",
    "                              input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4)))) \n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    #model.add(TimeDistributed(Dropout(0.25)))\n",
    "                                      \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "                                      \n",
    "    model.add(LSTM(32))\n",
    "                                      \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.client = pymongo.MongoClient(self.config.db_host)\n",
    "        self.db = self.client[self.config.db_name]\n",
    "\n",
    "    def load_video_data(self):\n",
    "        # Implement logic to load video data from MongoDB\n",
    "        pass\n",
    "\n",
    "class BaseModelBuilder:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def build_base_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.config.input_shape),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.config.num_classes, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def preprocess_video(self, video_data):\n",
    "        frames = []\n",
    "        for frame_data in video_data:\n",
    "            # Load frame from video_data\n",
    "            frame = cv2.imread(frame_data['frame_path'])\n",
    "            # Resize frame to match model input shape\n",
    "            frame = cv2.resize(frame, (self.config.input_shape[0], self.config.input_shape[1]))\n",
    "            # Perform normalization or other preprocessing steps if necessary\n",
    "            frame = frame / 255.0  # Example normalization\n",
    "            frames.append(frame)\n",
    "        return frames\n",
    "\n",
    "class VideoTrainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self, train_data, validation_data, model):\n",
    "        # Implement training logic using train_data and validation_data\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, test_data, model):\n",
    "        # Implement evaluation logic using test_data\n",
    "        pass\n",
    "\n",
    "    def get_model_summary(self, model):\n",
    "        return model.summary()\n",
    "\n",
    "def main():\n",
    "    # Initialize components\n",
    "    config = load_config()\n",
    "    data_ingestion = DataIngestion(config)\n",
    "    base_model_builder = BaseModelBuilder(config)\n",
    "    data_preprocessor = DataPreprocessor(config)\n",
    "    video_trainer = VideoTrainer(config)\n",
    "\n",
    "    # Load video data from MongoDB\n",
    "    video_data = data_ingestion.load_video_data()\n",
    "\n",
    "    # Preprocess video data\n",
    "    preprocessed_data = data_preprocessor.preprocess_video(video_data)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    train_data, val_data = train_test_split(preprocessed_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build base model\n",
    "    model = base_model_builder.build_base_model()\n",
    "\n",
    "    # Train model\n",
    "    video_trainer.train(train_data, val_data, model)\n",
    "\n",
    "    # Evaluate model\n",
    "    test_data = load_test_data()  # Load test data from somewhere\n",
    "    video_trainer.evaluate(test_data, model)\n",
    "\n",
    "    # Get model summary\n",
    "    model_summary = video_trainer.get_model_summary(model)\n",
    "    print(model_summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"college-project\"\n",
    "DB_HOST = \"mongodb+srv://niral0901:d3gAL01dwlOnqmLQ@college-project.iwlsqpp.mongodb.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongoengine import disconnect, connect, Document, StringField, IntField\n",
    "\n",
    "# Define MongoDB connection\n",
    "disconnect()\n",
    "connect(DB_NAME, host=DB_HOST)\n",
    "# Define MongoDB Document schema\n",
    "class LabelMapping(Document):\n",
    "    gloss = StringField(unique=True)\n",
    "    value = IntField()\n",
    "\n",
    "# Function to populate MongoDB with label mappings\n",
    "def populate_label_mappings(train_dir):\n",
    "    for index, gloss_label in enumerate(os.listdir(train_dir)):\n",
    "        LabelMapping(gloss=gloss_label, value=index+1).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def preprocess_video(video_path, target_size=(224, 224)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Couldn't open video file: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frame = frame / 255.0  # Normalize pixel values\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    if not frames:\n",
    "        print(f\"Error: No frames found in video file: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Find the maximum dimensions among all frames\n",
    "    max_height = max(frame.shape[0] for frame in frames)\n",
    "    max_width = max(frame.shape[1] for frame in frames)\n",
    "    \n",
    "    # Process frames to have the same dimensions\n",
    "    frames_processed = []\n",
    "    for frame in frames:\n",
    "        if frame.shape[0] < max_height:\n",
    "            pad_top = (max_height - frame.shape[0]) // 2\n",
    "            pad_bottom = max_height - frame.shape[0] - pad_top\n",
    "            frame = cv2.copyMakeBorder(frame, pad_top, pad_bottom, 0, 0, cv2.BORDER_CONSTANT, value=0)\n",
    "        elif frame.shape[0] > max_height:\n",
    "            crop_top = (frame.shape[0] - max_height) // 2\n",
    "            crop_bottom = crop_top + max_height\n",
    "            frame = frame[crop_top:crop_bottom, :]\n",
    "        \n",
    "        if frame.shape[1] < max_width:\n",
    "            pad_left = (max_width - frame.shape[1]) // 2\n",
    "            pad_right = max_width - frame.shape[1] - pad_left\n",
    "            frame = cv2.copyMakeBorder(frame, 0, 0, pad_left, pad_right, cv2.BORDER_CONSTANT, value=0)\n",
    "        elif frame.shape[1] > max_width:\n",
    "            crop_left = (frame.shape[1] - max_width) // 2\n",
    "            crop_right = crop_left + max_width\n",
    "            frame = frame[:, crop_left:crop_right]\n",
    "        \n",
    "        frames_processed.append(frame)\n",
    "    \n",
    "    # Pad or crop frames to a fixed number of frames\n",
    "    num_frames = 16\n",
    "    if len(frames_processed) < num_frames:\n",
    "        padding = [np.zeros((max_height, max_width, 3))] * (num_frames - len(frames_processed))\n",
    "        frames_processed.extend(padding)\n",
    "    elif len(frames_processed) > num_frames:\n",
    "        frames_processed = frames_processed[:num_frames]\n",
    "    \n",
    "    video = np.stack(frames_processed)\n",
    "    return video\n",
    "\n",
    "\n",
    "# Function to generate batches of video data\n",
    "def video_data_generator(train_dir, batch_size):\n",
    "    # Query MongoDB for label mappings\n",
    "    label_mappings = {mapping.gloss: mapping.value for mapping in LabelMapping.objects}\n",
    "\n",
    "    while True:\n",
    "        video_paths = [os.path.join(train_dir, gloss_label, video_name) \n",
    "               for gloss_label in os.listdir(train_dir) if gloss_label != '.DS_Store'\n",
    "               for video_name in os.listdir(os.path.join(train_dir, gloss_label))]\n",
    "        np.random.shuffle(video_paths)\n",
    "        for i in range(0, len(video_paths), batch_size):\n",
    "            batch_video_paths = video_paths[i:i+batch_size]\n",
    "            batch_videos = [preprocess_video(video_path) for video_path in batch_video_paths]\n",
    "            batch_videos = [video for video in batch_videos if video is not None]  # Remove None values\n",
    "            if not batch_videos:\n",
    "                continue\n",
    "            batch_labels = [os.path.basename(video_path).split('_')[0] for video_path in batch_video_paths]  # Extract gloss labels\n",
    "            batch_labels = [label_mappings.get(label, 0) for label in batch_labels]  # Map gloss labels to numerical values\n",
    "            batch_labels = to_categorical(batch_labels, num_classes=num_classes)\n",
    "            yield np.array(batch_videos), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'artifacts/data_ingestion'\n",
    "val_dir = 'artifacts/data_ingestion'\n",
    "\n",
    "# Define batch size and number of classes\n",
    "batch_size = 4\n",
    "num_classes = 51  # Number of sign classes\n",
    "\n",
    "# Create generators for training and validation data\n",
    "train_generator = video_data_generator(train_dir, batch_size)\n",
    "test_generator = video_data_generator(val_dir, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Couldn't open video file: artifacts/data_ingestion/white/white_63211.mp4\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x3edfd92f0] moov atom not found\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/white/white_63211.mp4\"\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x39838ab50] moov atom not found\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/yes/yes_64295.mp4\"\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x3465f0ac0] moov atom not found\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/hearing/hearing_26978.mp4\"\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x346b5c580] moov atom not found\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/fine/fine_65717.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Couldn't open video file: artifacts/data_ingestion/yes/yes_64295.mp4\n",
      "Error: Couldn't open video file: artifacts/data_ingestion/hearing/hearing_26978.mp4\n",
      "Error: Couldn't open video file: artifacts/data_ingestion/fine/fine_65717.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x345d05a60] moov atom not found\n",
      "OpenCV: Couldn't read video stream from file \"artifacts/data_ingestion/like/like_33281.mp4\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Couldn't open video file: artifacts/data_ingestion/like/like_33281.mp4\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/cn/59_k8cws6g95xx26rdq842z80000gn/T/ipykernel_39002/1566937283.py\", line 31, in <module>\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[3,51] labels_size=[4,51]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_174834]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m sign_language_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43msign_language_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m sign_language_model\u001b[38;5;241m.\u001b[39mevaluate(test_generator)\n",
      "File \u001b[0;32m~/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/cn/59_k8cws6g95xx26rdq842z80000gn/T/ipykernel_39002/1566937283.py\", line 31, in <module>\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n\n  File \"/Users/niralpatel/Desktop/Projects/speaking_silence/env/lib/python3.9/site-packages/keras/src/backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[3,51] labels_size=[4,51]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_174834]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model architecture\n",
    "def create_sign_language_model(num_classes):\n",
    "    # CNN backbone\n",
    "    cnn_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Temporal processing for each frame\n",
    "    video_input = layers.Input(shape=(None, 224, 224, 3))  # Input shape: (batch_size, num_frames, height, width, channels)\n",
    "    frame_features = layers.TimeDistributed(cnn_model)(video_input)\n",
    "    flattened_features = layers.TimeDistributed(layers.Flatten())(frame_features)\n",
    "    \n",
    "    # LSTM layer for temporal processing\n",
    "    lstm_output = layers.LSTM(128)(flattened_features)\n",
    "    \n",
    "    # Classification head\n",
    "    output = layers.Dense(num_classes, activation='softmax')(lstm_output)\n",
    "    \n",
    "    model = models.Model(inputs=video_input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = 51  # Number of sign classes\n",
    "sign_language_model = create_sign_language_model(num_classes)\n",
    "\n",
    "# Compile the model\n",
    "sign_language_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = sign_language_model.fit(train_generator, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = sign_language_model.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
